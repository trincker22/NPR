---
title: "NPR Project"
output: html_document
date: "2025-05-01"
---

```{r setup, include = FALSE}


rm(list = ls())

library(tidyverse)
library(readr)
library(jsonlite)
library(purrr)
library(lubridate)
library(quanteda)  
library(here)
library(tokenizers)
library(topicmodels)
library(syuzhet)
library(caret)
library(irr)



# pull files 
csvfiles <- list.files(path = here("archive"), pattern = "\\.csv$", full.names = TRUE)
jsonfiles <- list.files(path = here("archive"), pattern = "\\.json$", full.names = TRUE)

names(csvfiles) <- make.names(tools::file_path_sans_ext(basename(csvfiles)))
names(jsonfiles) <- make.names(tools::file_path_sans_ext(basename(jsonfiles)))

# load files w anon fuction 
walk(names(csvfiles), ~ assign(.x, read_csv(csvfiles[[.x]]), envir = .GlobalEnv))
walk(names(jsonfiles), ~ assign(.x, fromJSON(jsonfiles[[.x]], flatten = TRUE), envir = .GlobalEnv))


```

# Cleaning

```{r, results = "hide"}

# collapse to 1 ob per ep
ep_utterances <- utterances.2sp %>%
  group_by(episode) %>%
  summarise(full_text = paste(utterance, collapse = " "), .groups = "drop")

# collapse but w only guest speech
ep_utterancesG <- utterances.2sp %>%
  filter(!is_host) %>%
  group_by(episode) %>%
  summarise(full_text = paste(utterance, collapse = " "), .groups = "drop")

# host id extract & pull metadata from list 
host_id2sp <- utterances.2sp %>%
  filter(host_id != -1) %>%
  distinct(episode, host_id)

host_episode_df <- map_dfr(host.map, ~ tibble(
  episode = .x$episodes,
  host_name = .x$name
))

# function for joins w out redundancy 
safe_join <- function(df, join_df, by_col, new_col) {
  if (!(new_col %in% colnames(df))) {
    df <- df %>% left_join(join_df, by = by_col)
  } else {
    stop(glue::glue("'{new_col}' already exists"))
  }
  return(df)
}


# preprocessing loop for df w/wout the host speech (but w host metadata)
for (df_name in c("ep_utterances", "ep_utterancesG")) {
  df <- get(df_name)

  df <- safe_join(df, host_id2sp, "episode", "host_id")
  df <- safe_join(df, host_episode_df, "episode", "host_name")
  df <- safe_join(df, episodes, join_by(episode == id), "title")

  # program as factor for vis 
  if ("program" %in% colnames(df)) {
    df$program <- as.factor(df$program)
  }

  # date processing w lubridate
  if ("episode_date" %in% colnames(df)) {
    df <- df %>%
      mutate(
        episode_date = as.Date(episode_date),
        year = year(episode_date),
        month = month(episode_date, label = TRUE),
        weekday = wday(episode_date, label = TRUE)
      )
  }

  assign(df_name, df, envir = .GlobalEnv)
}


```

# Preprocess Text

```{r}


# corpus and tokenize
ep_uttercorp <- corpus(ep_utterances, text_field = "full_text")

toks <- tokens(ep_uttercorp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_wordstem() %>%
  tokens_select(pattern = stopwords("en"), selection = "remove")

# create and trim dfm (high freq bar)
dfm <- dfm(toks)
dfm_trimmed <- dfm_trim(dfm, min_termfreq = 50)

# sampling so i can run/not too large
set.seed(123)
sample_size <- 10000
sampled_rows <- sample(seq_len(ndoc(dfm_trimmed)), sample_size)
dfm_sampled <- dfm_trimmed[sampled_rows, ]

# convert to topicmodels format and run LDA
dtm <- convert(dfm_sampled, to = "topicmodels")
# lda_model <- LDA(dtm, k = 20, method = "Gibbs", control = list(seed = 1234))
# takes foreverrr to run 

# terms(lda_model, 20)
# topic_weights <- posterior(lda_model)$topics


```

# Process & LDA Model For 2 Person Interviews Discussing Immigration

```{r}


# define immigration-related stems & filter 
immigration_stems <- c(
  "immigr", "migrat", "migrant", "refuge", "asylum",
  "deport", "citizenship", "visa", "undocu"
)

immigration_stems_LDA <- c(
  "immigr", "migrat", "migrant", "refuge", "asylum",
  "deport", "citizenship", "visa", "undocu", "natur"
)

doc_index <- rowSums(dfm_select(dfm, pattern = immigration_stems_LDA)) > 0
doc_index_new <- rowSums(dfm_select(dfm, pattern = immigration_stems)) > 0
dfm_filtered <- dfm[doc_index, ]

dfm_immig <- dfm_trim(dfm_filtered, min_termfreq = 20)

# convert to topicmodels format and run lda
dtmIM <- convert(dfm_immig, to = "topicmodels")

if (!file.exists("lda_IMmodel.rds")) {
  lda_IMmodel <- LDA(dtmIM, k = 4, method = "Gibbs", control = list(seed = 1234))
  saveRDS(lda_IMmodel, "lda_IMmodel.rds")
} else {
  lda_IMmodel <- readRDS("lda_IMmodel.rds")
}

terms(lda_IMmodel, 20)

# get topic weights per doc
topic_weights <- posterior(lda_IMmodel)$topics
dominant_topic <- apply(topic_weights, 1, which.max)

# get matching metadata to convert back to df 
ep_immigration <- ep_utterances[doc_index, ]
topic_df <- data.frame(topic_weights)
colnames(topic_df) <- as.character(seq_len(ncol(topic_df)))


# assign 4-month period for aggregation

# ok, so i changed the df size to 2k obs, i dont remember what did but i made the filtering more 
#stringent. 
topic_df$period <- floor_date(ep_immigration$episode_date, "4 months")

# reshape and average topic weights
topic_time <- topic_df %>%
  pivot_longer(cols = c("1", "2", "3", "4"), names_to = "topic", values_to = "weight") %>%
  group_by(period, topic) %>%
  summarise(avg_weight = mean(weight), .groups = "drop")

# define topic labels
topic_labels <- c(
  "1" = "Political framing",
  "2" = "Personal stories",
  "3" = "Security & foreign affairs",
  "4" = "Public debate"
)

# map topic numbers to labels
topic_time <- topic_time %>%
  mutate(topic_label = factor(topic_labels[topic], levels = topic_labels))


```

# Visualization

### Immigration Topic Trends Over Time

```{r}

# plot
ggplot(topic_time, aes(x = period, y = avg_weight, color = topic_label)) +
  geom_line(linewidth = 0.7) +
  labs(title = "Immigration Topic Trends Over Time",
       x = "Month", y = "Average Topic Proportion",
       color = "Topic") +
  theme_minimal()

```

### Immigration as Share of Episodes

```{r}

ep_immigration_topics <- ep_immigration %>%
  mutate(dominant_topic = dominant_topic) %>%
  bind_cols(as.data.frame(topic_weights))


ep_immigration_topics <- ep_immigration_topics %>%
  mutate(topic_label = topic_labels[as.character(dominant_topic)])

# immigration share of all episodes over time (4-month periods)
ep_immigration_topics %>%
  mutate(period = floor_date(episode_date, "4 months")) %>%
  count(period, name = "immig_episodes") %>%
  left_join(episodes %>%
              mutate(period = floor_date(episode_date, "4 months")) %>%
              count(period, name = "total_episodes"),
            by = "period") %>%
  mutate(share = immig_episodes / total_episodes) %>%
  ggplot(aes(x = period, y = share)) +
  geom_line(size = 0.7) +
  labs(title = "Immigration Coverage as % of All Episodes Over Time",
       x = "Date", y = "Percent of Episodes") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal()

```

### Immigration Coverage By Program

```{r}
# immigration coverage as % of all episodes per program
ep_immigration_topics %>%
  count(program, name = "immig_episodes") %>%
  left_join(episodes %>% count(program, name = "total_episodes"), by = "program") %>%
  mutate(immig_share = immig_episodes / total_episodes) %>%
  filter(total_episodes >= 10) %>%  # optional: drop very small shows
  slice_max(immig_share, n = 10) %>%
  ggplot(aes(x = reorder(program, immig_share), y = immig_share)) +
  geom_col(fill = "steelblue") +
  labs(title = "Immigration coverage as % of all episodes",
       x = "Program", y = "Percent immigration-related") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  coord_flip() +
  theme_minimal()

```

### Sentiment By Immigration Framing Topic

```{r}
# sentiment by topic 
ep_immigration_topics$sentiment <- get_sentiment(ep_immigration_topics$full_text, method = "afinn")

ep_immigration_topics %>%
  group_by(topic_label) %>%
  summarise(avg_sentiment = mean(sentiment, na.rm = TRUE)) %>%
  ggplot(aes(x = reorder(topic_label, avg_sentiment), y = avg_sentiment, fill = topic_label)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Average sentiment by topic",
       x = "Topic", y = "Avg sentiment score") +
  coord_flip() +
  theme_minimal()


```

### Host Sentiment By Framing Topic

```{r}

top_hosts <- ep_immigration_topics %>%
  count(host_name, sort = TRUE) %>%
  slice_head(n = 5) %>%
  pull(host_name)

top_host_data <- ep_immigration_topics %>%
  filter(host_name %in% top_hosts)


top_host_data %>%
  group_by(host_name, topic_label) %>%
  summarise(avg_sentiment = mean(sentiment, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = topic_label, y = avg_sentiment, fill = host_name)) +
  geom_col(position = "dodge") +
  labs(title = "Average Sentiment by Topic and Host",
       x = "Topic", y = "Average Sentiment",
       fill = "Host") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

```

### Host Framing Topic Distribution

```{r}

top_host_data %>%
  count(host_name, topic_label) %>%
  group_by(host_name) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = host_name, y = prop, fill = topic_label)) +
  geom_col(position = "fill") +
  labs(title = "Topic Distribution by Top 5 Hosts",
       x = "Host", y = "Proportion of Episodes",
       fill = "Topic") +
  theme_minimal()


```

#Pull ~100 Word Snippet 


```{r}


ep_immigration <- ep_utterances[doc_index_new, ]

# handcoding can easily be joined using episode number, a unique observation identifier. 

immigration_stems <- c(
  "immigr", "migrat", "migrant", "refuge", "asylum",
  "deport", "citizenship", "visa", "undocu"
)

vec <- ep_immigration %>% 
  select(full_text) %>% 
  pull()

pattern <- paste0(immigration_stems, collapse = "|")

# Function: One snippet per episode
get_one_snippet <- function(text, pattern, window = 50) {
  words <- unlist(tokenize_words(text, lowercase = TRUE))
  matches <- which(str_detect(words, pattern))
  
  if (length(matches) == 0) return(NA_character_)  # no keyword
  
  # Build Â±window for each match
  windows <- lapply(matches, function(i) c(max(1, i - window), min(length(words), i + window)))
  
  # Merge overlapping windows
  windows <- windows[order(sapply(windows, `[[`, 1))]  # sort by start index
  merged <- list()
  current <- windows[[1]]
  
  for (w in windows[-1]) {
    if (w[1] <= current[2]) {
      current[2] <- max(current[2], w[2])  # extend current window
    } else {
      merged <- append(merged, list(current))
      current <- w
    }
  }
  merged <- append(merged, list(current))
  
  # Choose one merged snippet: random if multiple
  chosen <- merged[[sample(length(merged), 1)]]
  snippet <- paste(words[chosen[1]:chosen[2]], collapse = " ")
  return(snippet)
}


snippets <- sapply(vec, get_one_snippet, pattern = pattern)

snippets_df <- data.frame(
  index = seq_along(snippets),
  snippet = snippets,
  stringsAsFactors = FALSE
)

if(!"snippet" %in% colnames(immigration)) {
immigration <- cbind(snippets_df, ep_immigration)
} else {
  paste("stop")
}

immigration %>% 
  select(-index)

```

# Inter-coder Reliability 



```{r}

hc_complete <- read.csv("HandcodeComplete.csv")

all_categories <- c("Security.Threat", "Economic", "Humanitarian.Moral", "Other")


cols_1 <- paste0(all_categories, "1")
cols_2 <- paste0(all_categories, "2")

# ensure one-hot columns are numeric
hc_complete[cols_1] <- lapply(hc_complete[cols_1], as.numeric)
hc_complete[cols_2] <- lapply(hc_complete[cols_2], as.numeric)

#function to extract label from one-hot columns
get_label <- function(row, suffix) {
  selected <- which(row[paste0(all_categories, suffix)] == 1)
  if (length(selected) == 1) {
    return(all_categories[selected])
  } else {
    return(NA)
  }
}

hc_complete$coder1_label <- apply(hc_complete, 1, get_label, suffix = "1")
hc_complete$coder2_label <- apply(hc_complete, 1, get_label, suffix = "2")

hc_complete$coder1_label <- factor(hc_complete$coder1_label, levels = all_categories)
hc_complete$coder2_label <- factor(hc_complete$coder2_label, levels = all_categories)

# confusion matrix
confusion_matrix <- table(hc_complete$coder1_label, hc_complete$coder2_label)
print(confusion_matrix)


#################################################
# making matrix for kripp alpha 

valid_categories <- c("Security.Threat", "Economic", "Humanitarian.Moral", "Other")
category_lookup <- setNames(1:4, valid_categories)

hc_clean <- hc_complete %>%
  filter(coder1_label %in% valid_categories,
         coder2_label %in% valid_categories)

# convert labels to numeric using the lookup
rater_matrix <- data.frame(
  coder1 = category_lookup[hc_clean$coder1_label],
  coder2 = category_lookup[hc_clean$coder2_label]
)

# transpose and run kripp alpha
rater_matrix_t <- t(rater_matrix)
kripp.alpha(rater_matrix_t, method = "nominal")


```




```{r}