---
title: "NPR Project"
output: html_document
date: "2025-05-01"
---

```{r, results = 'hide', message = FALSE, warning = FALSE}


library(tidyverse)
library(readr)
library(jsonlite)
library(purrr)
library(lubridate)
library(quanteda)  
library(quanteda.textmodels)
library(here)
library(tokenizers)
library(topicmodels)
library(syuzhet)
library(caret)
library(irr)
library(SnowballC)
library(ellmer)
library(data.table)
library(e1071)
library(randomForest)
library(glmnet)



# pull files 
csvfiles <- list.files(path = here("archive"), pattern = "\\.csv$", full.names = TRUE)
jsonfiles <- list.files(path = here("archive"), pattern = "\\.json$", full.names = TRUE)

names(csvfiles) <- make.names(tools::file_path_sans_ext(basename(csvfiles)))
names(jsonfiles) <- make.names(tools::file_path_sans_ext(basename(jsonfiles)))

# load files w anon fuction 
walk(names(csvfiles), ~ assign(.x, read_csv(csvfiles[[.x]]), envir = .GlobalEnv))
walk(names(jsonfiles), ~ assign(.x, fromJSON(jsonfiles[[.x]], flatten = TRUE), envir = .GlobalEnv))

```

# Cleaning

```{r clean, results = "hide"}


# Collapse to 1 observation per episode 
ep_utterances <- utterances.2sp %>%
  group_by(episode) %>%
  summarise(full_text = paste(utterance, collapse = " "), .groups = "drop")

ep_utterancesG <- utterances.2sp %>%
  filter(!is_host) %>%
  group_by(episode) %>%
  summarise(full_text = paste(utterance, collapse = " "), .groups = "drop")

# Collapse host names to comma-separated string per episode
host_episode_df <- map_dfr(host.map, ~ tibble(
  episode = .x$episodes,
  host_name = .x$name
)) %>%
  group_by(episode) %>%
  summarise(host_name = paste(unique(host_name), collapse = ", "), .groups = "drop")


safe_join <- function(df, join_df, by_col, new_col) {
  if (!(new_col %in% colnames(df))) {
    df <- df %>% left_join(join_df, by = by_col)
  } else {
    stop(glue("'{new_col}' already exists"))
  }
  return(df)
}

# Loop through both datasets to attach host metadata
for (df_name in c("ep_utterances", "ep_utterancesG")) {
  df <- get(df_name)

  df <- safe_join(df, host_episode_df, "episode", "host_name")
  df <- safe_join(df, episodes, join_by(episode == id), "title")

  # Convert program to factor
  if ("program" %in% colnames(df)) {
    df$program <- as.factor(df$program)
  }

  if ("episode_date" %in% colnames(df)) {
    df <- df %>%
      mutate(
        episode_date = as.Date(episode_date),
        year = year(episode_date),
        month = lubridate::month(episode_date, label = TRUE),
        weekday = lubridate::wday(episode_date, label = TRUE)
      )
  }

  assign(df_name, df, envir = .GlobalEnv)
}


```

# Preprocess Text

```{r create dfm,  results = 'hide'}

# create dfm with episode IDs stored as docvars
ep_uttercorp <- corpus(ep_utterances, text_field = "full_text")
docvars(ep_uttercorp, "episode") <- ep_utterances$episode


toks <- tokens(ep_uttercorp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_wordstem() %>%
  tokens_select(pattern = stopwords("en"), selection = "remove")


dfm <- dfm(toks)



```

```{r immigration subset,  results = 'hide'}


immigration_stems <- c("immigr", "migrat", "migrant", "refuge", "asylum",
                       "deport", "citizenship", "visa", "undocu")


doc_index_new <- rowSums(dfm_select(dfm, pattern = immigration_stems)) > 0


matching_episodes <- docvars(dfm, "episode")[doc_index_new]

ep_immigration <- ep_utterances %>% filter(episode %in% matching_episodes)


```
# Snippet Build
```{r snippet create,  results = 'hide'}

# pulls one snippet per episode by keyword, merges text if 2 keywords have overlapping windows, selects snippet randomly if there are multiple non-overlapping keyword windows

pattern <- paste0(immigration_stems, collapse = "|")


get_one_snippet <- function(text, pattern, window = 50) {
  words <- unlist(tokenizers::tokenize_words(text, lowercase = TRUE))
  matches <- which(stringr::str_detect(words, pattern))

  if (length(matches) == 0) return(NA_character_)


  windows <- lapply(matches, function(i) c(max(1, i - window), min(length(words), i + window)))


  windows <- windows[order(sapply(windows, `[[`, 1))]
  merged <- list()
  current <- windows[[1]]

  for (w in windows[-1]) {
    if (w[1] <= current[2]) {
      current[2] <- max(current[2], w[2])  
    } else {
      merged <- append(merged, list(current))
      current <- w
    }
  }
  merged <- append(merged, list(current))


  chosen <- merged[[sample(length(merged), 1)]]
  snippet <- paste(words[chosen[1]:chosen[2]], collapse = " ")
  return(snippet)
}

snippets <- sapply(ep_immigration$full_text, get_one_snippet, pattern = pattern)


snippets_df <- data.frame(
  episode = ep_immigration$episode,
  snippet = snippets,
  stringsAsFactors = FALSE
)


immigration <- left_join(snippets_df, ep_immigration, join_by(episode))


```

# Inter-coder Reliability

```{r intercoder reliability }

invisible(hc_complete <- read_csv("hc_complete.csv", show_col_types = FALSE))

all_categories <- c("Security.Threat", "Economic", "Humanitarian.Moral", "Other")


cols_1 <- paste0(all_categories, "1")
cols_2 <- paste0(all_categories, "2")


hc_complete[cols_1] <- lapply(hc_complete[cols_1], as.numeric)
hc_complete[cols_2] <- lapply(hc_complete[cols_2], as.numeric)

get_label <- function(row, suffix) {
  selected <- which(row[paste0(all_categories, suffix)] == 1)
  if (length(selected) == 1) {
    return(all_categories[selected])
  } else {
    return(NA)
  }
}

hc_complete$coder1_label <- apply(hc_complete, 1, get_label, suffix = "1")
hc_complete$coder2_label <- apply(hc_complete, 1, get_label, suffix = "2")

hc_complete$coder1_label <- factor(hc_complete$coder1_label, levels = all_categories)
hc_complete$coder2_label <- factor(hc_complete$coder2_label, levels = all_categories)

# confusion matrix
confusion_matrix <- table(hc_complete$coder1_label, hc_complete$coder2_label)
print(confusion_matrix)


#################################################
# making matrix for kripp alpha 

valid_categories <- c("Security.Threat", "Economic", "Humanitarian.Moral", "Other")
category_lookup <- setNames(1:4, valid_categories)

hc_clean <- hc_complete %>%
  filter(coder1_label %in% valid_categories,
         coder2_label %in% valid_categories)

# convert labels to numeric using the lookup
rater_matrix <- data.frame(
  coder1 = category_lookup[hc_clean$coder1_label],
  coder2 = category_lookup[hc_clean$coder2_label]
)

# transpose and run kripp alpha
rater_matrix_t <- t(rater_matrix)
kripp.alpha(rater_matrix_t, method = "nominal")


```

# Bayes Classifier

```{r bayes}

if (!"snippet" %in% names(hc_complete)) {
  hc_complete <- hc_complete %>%
    left_join(
      immigration %>% select(episode, snippet),
      by = c("matched_episode" = "episode")
    )
}

set.seed(123)
n <- nrow(hc_complete)

full_corp <- corpus(hc_complete, text_field = "snippet")
full_toks <- tokens(full_corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en")) %>%
  tokens_ngrams(n = 1:2)

full_dfm <- dfm(full_toks) %>%
  dfm_trim(min_termfreq = 5) %>%
  dfm_weight(scheme = "prop")

true_labels <- factor(hc_complete$coder1_label)

precision_list <- list()
recall_list <- list()
accuracy_vec <- numeric(n)
all_predictions <- character(n)

for (i in 1:n) {
  train_dfm <- full_dfm[-i, ]
  test_dfm <- full_dfm[i, , drop = FALSE]
  train_labels <- true_labels[-i]
  test_label <- true_labels[i]
  test_dfm <- dfm_match(test_dfm, featnames(train_dfm))
  nb_model <- textmodel_nb(train_dfm, y = train_labels)
  pred <- predict(nb_model, newdata = test_dfm)
  all_predictions[i] <- as.character(unname(pred))
  pred_factor <- factor(pred, levels = levels(true_labels))
  test_factor <- factor(test_label, levels = levels(true_labels))
  conf <- confusionMatrix(pred_factor, test_factor)
  precision_list[[i]] <- conf$byClass[, "Precision"]
  recall_list[[i]] <- conf$byClass[, "Recall"]
  accuracy_vec[i] <- conf$overall["Accuracy"]
}

precision_mat <- do.call(rbind, precision_list)
recall_mat <- do.call(rbind, recall_list)
avg_precision <- colMeans(precision_mat, na.rm = TRUE)
avg_recall <- colMeans(recall_mat, na.rm = TRUE)
avg_accuracy <- mean(accuracy_vec, na.rm = TRUE)

cat("\nLOOCV Average Metrics\n")
cat("Average Accuracy:", round(avg_accuracy, 4), "\n\n")
cat("Average Precision per class:\n")
print(round(avg_precision, 4))
cat("\nAverage Recall per class:\n")
print(round(avg_recall, 4))

```


```{r}


set.seed(124)
n <- nrow(hc_complete)


glove_file <- "glove.twitter.27B.100d.txt"
glove_raw <- fread(
  glove_file,
  header = FALSE,
  quote = "",
  colClasses = c("character", rep("numeric", 100))
)
setnames(glove_raw, c("word", paste0("V", 1:100)))


tokenized <- strsplit(tolower(hc_complete$snippet), "\\s+")

doc_embeddings <- t(sapply(tokenized, function(words) {
  matched <- glove_raw[glove_raw$word %in% words]
  if (nrow(matched) > 0) {
    colMeans(as.matrix(matched[, -1, with = FALSE]))
  } else {
    rep(0, 100)
  }
}))

true_labels <- factor(hc_complete$coder1_label)

precision_list <- list()
recall_list <- list()
accuracy_vec <- numeric(n)

for (i in 1:n) {
  train_x <- doc_embeddings[-i, ]
  test_x <- matrix(doc_embeddings[i, ], nrow = 1)
  
  train_y <- true_labels[-i]
  test_y <- true_labels[i]
  
  nb_model <- naiveBayes(x = train_x, y = train_y)
  
  pred <- predict(nb_model, newdata = test_x)
  
  pred_factor <- factor(pred, levels = levels(true_labels))
  test_factor <- factor(test_y, levels = levels(true_labels))
  
  conf <- confusionMatrix(pred_factor, test_factor)
  
  precision_list[[i]] <- conf$byClass[, "Precision"]
  recall_list[[i]] <- conf$byClass[, "Recall"]
  accuracy_vec[i] <- conf$overall["Accuracy"]
}

precision_mat <- do.call(rbind, precision_list)
recall_mat <- do.call(rbind, recall_list)
avg_precision <- colMeans(precision_mat, na.rm = TRUE)
avg_recall <- colMeans(recall_mat, na.rm = TRUE)
avg_accuracy <- mean(accuracy_vec, na.rm = TRUE)

cat("\nLOOCV Average Metrics (Naive Bayes, raw Twitter GloVe)\n")
cat("Average Accuracy:", round(avg_accuracy, 4), "\n\n")
cat("Average Precision per class:\n")
print(round(avg_precision, 4))
cat("\nAverage Recall per class:\n")
print(round(avg_recall, 4))



```

# Support Vector Machine 
With 100D Twitter Embeddings, class weighting. 

```{r}

set.seed(124)
n <- nrow(hc_complete)


true_labels <- factor(hc_complete$coder1_label)

precision_list <- list()
recall_list <- list()
accuracy_vec <- numeric(n)

for (i in 1:n) {
  train_x <- doc_embeddings[-i, ]
  test_x <- matrix(doc_embeddings[i, ], nrow = 1)
  
  train_y <- true_labels[-i]
  test_y <- true_labels[i]
  
  class_counts <- table(train_y)
  inv_weights <- sum(class_counts) / (length(class_counts) * class_counts)
  inv_weights <- as.list(inv_weights)
  names(inv_weights) <- names(class_counts)
  
  svm_model <- svm(
    x = train_x,
    y = train_y,
    kernel = "linear",
    class.weights = inv_weights
  )
  
  pred <- predict(svm_model, test_x)
  
  pred_factor <- factor(pred, levels = levels(true_labels))
  test_factor <- factor(test_y, levels = levels(true_labels))
  
  conf <- confusionMatrix(pred_factor, test_factor)
  
  precision_list[[i]] <- conf$byClass[, "Precision"]
  recall_list[[i]] <- conf$byClass[, "Recall"]
  accuracy_vec[i] <- conf$overall["Accuracy"]
}

precision_mat <- do.call(rbind, precision_list)
recall_mat <- do.call(rbind, recall_list)
avg_precision <- colMeans(precision_mat, na.rm = TRUE)
avg_recall <- colMeans(recall_mat, na.rm = TRUE)
avg_accuracy <- mean(accuracy_vec, na.rm = TRUE)

cat("\nLOOCV Average Metrics (Weighted SVM, raw Twitter GloVe)\n")
cat("Average Accuracy:", round(avg_accuracy, 4), "\n\n")
cat("Average Precision per class:\n")
print(round(avg_precision, 4))
cat("\nAverage Recall per class:\n")
print(round(avg_recall, 4))



```
PCA to reduce dimensionality. 
```{R}
set.seed(124)
n <- nrow(hc_complete)

pca <- prcomp(doc_embeddings, center = TRUE, scale. = TRUE)
reduced_embeddings <- pca$x[, 1:30]


# Looking at the PCA. it actually is very poor 
pca_df <- data.frame(
  PC1 = pca$x[, 1],
  PC2 = pca$x[, 2],
  label = hc_complete$coder1_label
)


ggplot(pca_df, aes(x = PC1, y = PC2, color = label)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(title = "PCA Plot of Twitter GloVe Embeddings",
       x = "Principal Component 1",
       y = "Principal Component 2",
       color = "Class Label") +
  theme_minimal()


######################

true_labels <- factor(hc_complete$coder1_label)

precision_list <- list()
recall_list <- list()
accuracy_vec <- numeric(n)

for (i in 1:n) {
  train_x <- reduced_embeddings[-i, ]
  test_x <- matrix(reduced_embeddings[i, ], nrow = 1)
  
  train_y <- true_labels[-i]
  test_y <- true_labels[i]
  
  class_counts <- table(train_y)
  inv_weights <- sum(class_counts) / (length(class_counts) * class_counts)
  inv_weights <- as.list(inv_weights)
  names(inv_weights) <- names(class_counts)
  
  svm_model <- svm(
    x = train_x,
    y = train_y,
    kernel = "linear",
    class.weights = inv_weights
  )
  
  pred <- predict(svm_model, test_x)
  
  pred_factor <- factor(pred, levels = levels(true_labels))
  test_factor <- factor(test_y, levels = levels(true_labels))
  
  conf <- confusionMatrix(pred_factor, test_factor)
  
  precision_list[[i]] <- conf$byClass[, "Precision"]
  recall_list[[i]] <- conf$byClass[, "Recall"]
  accuracy_vec[i] <- conf$overall["Accuracy"]
}

precision_mat <- do.call(rbind, precision_list)
recall_mat <- do.call(rbind, recall_list)
avg_precision <- colMeans(precision_mat, na.rm = TRUE)
avg_recall <- colMeans(recall_mat, na.rm = TRUE)
avg_accuracy <- mean(accuracy_vec, na.rm = TRUE)

cat("\n=== LOOCV Average Metrics (Weighted SVM, No PCA)\n")
cat("Average Accuracy:", round(avg_accuracy, 4), "\n\n")
cat("Average Precision per class:\n")
print(round(avg_precision, 4))
cat("\nAverage Recall per class:\n")
print(round(avg_recall, 4))

```

# LASSO

```{r}

set.seed(123)
n <- nrow(hc_complete)

true_labels <- factor(hc_complete$coder1_label)

# Find best lambda on full data (unweighted)
full_lasso_cv <- cv.glmnet(
  x = doc_embeddings,
  y = as.numeric(true_labels),
  family = "multinomial",
  alpha = 1
)

best_lambda <- full_lasso_cv$lambda.min
cat("Best lambda from full CV:", best_lambda, "\n")

all_predictions <- character(n)

for (i in 1:n) {
  train_x <- doc_embeddings[-i, ]
  test_x <- matrix(doc_embeddings[i, ], nrow = 1)
  
  train_y <- true_labels[-i]
  
  # inverse weights for weighted model 
  class_counts <- table(train_y)
  inv_weights <- 1 / class_counts[train_y]
  sample_weights <- as.numeric(inv_weights)
  
  lasso_model <- glmnet(
    x = train_x,
    y = as.numeric(train_y),
    family = "multinomial",
    alpha = 1,
    lambda = best_lambda,
    weights = sample_weights
  )
  
  pred <- predict(lasso_model, newx = test_x, type = "class")
  pred_class <- levels(true_labels)[as.numeric(pred)]
  all_predictions[i] <- pred_class
}

predicted_labels <- factor(all_predictions, levels = levels(true_labels))
conf <- confusionMatrix(predicted_labels, true_labels)

cat("\n=== LOOCV Metrics (Weighted LASSO) ===\n")
cat("Overall Accuracy:", round(conf$overall["Accuracy"], 4), "\n\n")
cat("Precision and Recall per class:\n")
print(round(conf$byClass[, c("Precision", "Recall")], 4))


```

# Random Forest (with raw embeddings)

```{r}

set.seed(124)
n <- nrow(hc_complete)


true_labels <- factor(hc_complete$coder1_label)

precision_list <- list()
recall_list <- list()
accuracy_vec <- numeric(n)

for (i in 1:n) {
  train_x <- doc_embeddings[-i, ]
  test_x <- matrix(doc_embeddings[i, ], nrow = 1)
  
  train_y <- true_labels[-i]
  test_y <- true_labels[i]
  
  # Upsample minority classes inside each fold
  train_data <- data.frame(train_x)
  train_data$label <- train_y
  
  set.seed(123)
  balanced_data <- upSample(x = train_data[, -ncol(train_data)], y = train_data$label)
  
  rf_model <- randomForest(
    x = balanced_data[, -ncol(balanced_data)],
    y = balanced_data$Class,
    ntree = 500
  )
  
  pred <- predict(rf_model, test_x)
  
  pred_factor <- factor(pred, levels = levels(true_labels))
  test_factor <- factor(test_y, levels = levels(true_labels))
  
  conf <- confusionMatrix(pred_factor, test_factor)
  
  precision_list[[i]] <- conf$byClass[, "Precision"]
  recall_list[[i]] <- conf$byClass[, "Recall"]
  accuracy_vec[i] <- conf$overall["Accuracy"]
}

precision_mat <- do.call(rbind, precision_list)
recall_mat <- do.call(rbind, recall_list)
avg_precision <- colMeans(precision_mat, na.rm = TRUE)
avg_recall <- colMeans(recall_mat, na.rm = TRUE)
avg_accuracy <- mean(accuracy_vec, na.rm = TRUE)

cat("\n=== LOOCV Average Metrics (Random Forest, raw Twitter GloVe) ===\n")
cat("Average Accuracy:", round(avg_accuracy, 4), "\n\n")
cat("Average Precision per class:\n")
print(round(avg_precision, 4))
cat("\nAverage Recall per class:\n")
print(round(avg_recall, 4))


```



```{r, eval = F}

#Batched prompt to classify entire immigration df 

chat <- ellmer::chat_google_gemini()

classify_immigration_batch <- function(text_batch) {
  Sys.sleep(15) 

  prompt <- paste(
    "Classify the following 3 immigration-related texts separately into one of four frames:\n\n",
    
    "Security/Threat Frame\n",
    "Articles that primarily frame immigration as a threat to national security, public safety, or the legal order. Even if a story seems sympathetic, if it evokes associations with security or crime (e.g., fleeing violence from cartels), classify as Security/Threat.\n",
    "Focus on illegal entry, smuggling, or border crossings\n",
    "Coverage of immigration enforcement (e.g., ICE raids, deportations)\n",
    "Crime or terrorism tied to immigrants or asylum seekers\n",
    "National security concerns or protective measures\n",
    "Political rhetoric focused on law and order\n",
    "Keywords: illegal, deportation, border wall, crime, ICE, security, terrorism, smuggling, cartels, public safety\n\n",
    
    "Economic Frame\n",
    "Articles that focus on the financial or labor market implications of immigration.\n",
    "Immigrants as workers in specific industries\n",
    "Central focus on appropriations/funding of immigration issues, even if the funding debate leads speakers to make arguments that fall into one of the other categories.\n",
    "Impact on wages, employment, housing, or taxes\n",
    "Economic contributions (e.g., entrepreneurship, GDP, remittances)\n",
    "Business demand for migrant labor or work visas\n",
    "Fiscal burden or benefit arguments\n",
    "Keywords: jobs, labor, taxes, contribution, costs, economic growth, shortages, employment, undocumented workers\n\n",
    
    "Humanitarian/Moral Frame\n",
    "Articles that highlight the lived experiences, rights, or moral standing of immigrants and refugees.\n",
    "Stories of asylum seekers, refugees, or displaced families\n",
    "Discussion of living conditions is the strongest predictor of this frame. Next;\n",
    "Focus on human suffering, trauma, or discrimination\n",
    "Coverage of family separation, detainment, or access to services\n",
    "Discussions of inclusion, dignity, or moral responsibility\n",
    "Civil society efforts to aid or defend migrant\n",
    "Keywords: asylum, refugee, family, rights, trauma, detention, inclusion, discrimination, DACA, compassion, migrant children\n\n",
    
    "Other\n",
    "Mention of immigration is offhand, not the major focus of the piece—for example, coverage of a session of congress where immigration is a topic of focus, but immigration is merely tangential. Or, discussion of presidential approval rates, and immigration is mentioned as a topic influencing the results, but is not the focus of the piece. Finally, snippets generally not relevant to other categories should fall here.\n",
    "Broad overview of political position/environment that only tangentially mentions immigration.\n\n",
    
    "Now classify these:\n",
    paste0("Text 1: ", text_batch[1], "\n"),
    paste0("Text 2: ", text_batch[2], "\n"),
    paste0("Text 3: ", text_batch[3], "\n"),
    "Respond ONLY with three lines:\n1) [frame]\n2) [frame]\n3) [frame]"
  )

  result <- tryCatch({
    response <- chat$chat(prompt)
    response_lines <- strsplit(response, "\n")[[1]]
    
    frames <- sapply(response_lines, function(line) {
      line_clean <- tolower(gsub("^\\d+\\)\\s*", "", line))
      if (grepl("security", line_clean)) {
        "Security/Threat"
      } else if (grepl("economic", line_clean)) {
        "Economic"
      } else if (grepl("humanitarian|moral", line_clean)) {
        "Humanitarian/Moral"
      } else if (grepl("other", line_clean)) {
        "Other"
      } else {
        "Unclear"
      }
    })

    return(frames)
    
  }, error = function(e) {
    message("API error on batch: ", e$message)
    return(rep(NA, 3))
  })
  
  return(result)
}

texts <- immigration$snippet

# Split into batches of 3
batches <- split(texts, ceiling(seq_along(texts) / 3))

# Load existing results
if (exists("final_results")) {
  processed_indices <- final_results$index
  start_batch <- ceiling(max(processed_indices) / 3) + 1
  cat("Resuming from batch", start_batch, "of", length(batches), "\n")
  
  all_results <- split(final_results, ceiling(final_results$index / 3))
} else {
  start_batch <- 1
  all_results <- list()
}


for (i in start_batch:length(batches)) {
  batch <- batches[[i]]
  
  # Ensure the batch has exactly 3 items 
  while (length(batch) < 3) {
    batch <- c(batch, "")
  }
  
  cat("Processing batch", i, "of", length(batches), "\n")
  
  frames <- classify_immigration_batch(batch)
  
  indices <- ((i - 1) * 3 + 1):min(i * 3, length(texts))
  
  all_results[[i]] <- data.frame(
    index = indices,
    frame = frames[1:length(indices)]  # trim extra if padded
  )

  
  if (i %% 10 == 0) {
    saveRDS(do.call(rbind, all_results), file = "checkpoint_results.rds")
    cat("Checkpoint saved at batch", i, "\n")
  }
}


immigration$gemini_label <- final_results$frame

summary(as.factor(final_results$frame))

# NA handling
na_indices <- final_results$index[is.na(final_results$frame)]

cat("Number of NA entries:", length(na_indices), "\n")


na_batches <- unique(ceiling(na_indices / 3))

cat("Number of batches to re-run:", length(na_batches), "\n")



for (i in na_batches) {
  batch <- batches[[i]]
  
  while (length(batch) < 3) {
    batch <- c(batch, "")
  }
  
  cat("Re-running batch", i, "of", length(batches), "\n")
  
  frames <- classify_immigration_batch(batch)
  
  indices <- ((i - 1) * 3 + 1):min(i * 3, length(texts))
  
  # Update final_results in place
  final_results$frame[final_results$index %in% indices] <- frames[1:length(indices)]
  
}

```



```{r}

immigration <- readRDS("immigration.rds")


hc_complete <- hc_complete %>% 
  inner_join(immigration %>% 
            select(gemini_label, episode), 
            join_by(matched_episode == episode))


true_labels <- factor(hc_complete$coder1_label)
predicted_labels <- factor(hc_complete$gemini_label, levels = levels(true_labels))

conf_matrix <- confusionMatrix(predicted_labels, true_labels)

conf_matrix

# Eval
print(conf_matrix$byClass[, c("Precision", "Recall")])



```
# VIS
```{r}

immigration$gemini_label <- gsub("Other", "Procedural", immigration$gemini_label)

topic_time <- immigration %>% 
  select(episode_date, gemini_label, episode) %>%  
  mutate(
    period = lubridate::floor_date(episode_date, "6 months")
  ) %>%  
  group_by(period) %>% 
  count(gemini_label) %>%  
  mutate( 
    shares = n/sum(n)
    )


# check 
topic_time %>%  
  group_by(period) %>% 
  summarise(sum(shares))


ggplot(topic_time, aes(x = period, y = shares, color = as.factor(gemini_label))) + 
  geom_line(size = 0.5, alpha = 0.8) +
  labs(title = "Immigration topic trends over time",
       x = "Month", y = "Average topic proportion",
       color = "Topic") +
  theme_minimal()


```


```{r}

immigration %>%
  count(program, name = "immig_episodes") %>%
  left_join(episodes %>% count(program, name = "total_episodes"), by = "program") %>%
  mutate(immig_share = immig_episodes / total_episodes) %>%
  ggplot(aes(x = reorder(program, immig_share), y = immig_share)) +
  geom_col(fill = "steelblue") +
  labs(title = "Immigration coverage as % of all episodes",
       x = "Program", y = "Percent immigration-related") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  coord_flip() +
  theme_minimal()

   # all programs have at least 10 episodes over the study period
```



```{r}
immigration$sentiment <- get_sentiment(immigration$snippet, method = "afinn")

immigration %>%
  group_by(gemini_label) %>%
  summarise(avg_sentiment = mean(sentiment, na.rm = TRUE)) %>%
  ggplot(aes(x = reorder(gemini_label, avg_sentiment), y = avg_sentiment, fill = gemini_label)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Average sentiment by topic",
       x = "Topic", y = "Avg sentiment score") +
  coord_flip() +
  theme_minimal()



```


```{r}

top_hosts <- immigration %>%
  count(host_name, sort = TRUE) %>%
  slice_head(n = 5) %>%
  pull(host_name)

top_host_data <- immigration %>%
  filter(host_name %in% top_hosts)


top_host_data %>%
  group_by(host_name, gemini_label) %>%
  summarise(avg_sentiment = mean(sentiment, na.rm = TRUE), .groups = "drop") %>%
  ggplot(aes(x = gemini_label, y = avg_sentiment, fill = host_name)) +
  geom_col(position = "dodge") +
  labs(title = "Average Sentiment by Topic and Host",
       x = "Topic", y = "Average Sentiment",
       fill = "Host") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))


```


```{r}

top_host_data %>%
  count(host_name, gemini_label) %>%
  group_by(host_name) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = host_name, y = prop, fill = gemini_label)) +
  geom_col(position = "fill") +
  labs(title = "Topic Distribution by Top 5 Hosts",
       x = "Host", y = "Proportion of Episodes",
       fill = "Topic") +
  theme_minimal()



```



