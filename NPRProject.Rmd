---
title: "NPR Project"
output: html_document
date: "2025-05-01"
---

```{r setup, include = FALSE}


rm(list = ls())

library(tidyverse)
library(readr)
library(jsonlite)
library(purrr)
library(lubridate)
library(quanteda)  
library(quanteda.textmodels)
library(here)
library(tokenizers)
library(topicmodels)
library(syuzhet)
library(caret)
library(irr)
library(SnowballC)




# pull files 
csvfiles <- list.files(path = here("archive"), pattern = "\\.csv$", full.names = TRUE)
jsonfiles <- list.files(path = here("archive"), pattern = "\\.json$", full.names = TRUE)

names(csvfiles) <- make.names(tools::file_path_sans_ext(basename(csvfiles)))
names(jsonfiles) <- make.names(tools::file_path_sans_ext(basename(jsonfiles)))

# load files w anon fuction 
walk(names(csvfiles), ~ assign(.x, read_csv(csvfiles[[.x]]), envir = .GlobalEnv))
walk(names(jsonfiles), ~ assign(.x, fromJSON(jsonfiles[[.x]], flatten = TRUE), envir = .GlobalEnv))


```

# Cleaning

```{r, results = "hide"}


# Collapse to 1 observation per episode (full transcript and guest-only)
ep_utterances <- utterances.2sp %>%
  group_by(episode) %>%
  summarise(full_text = paste(utterance, collapse = " "), .groups = "drop")

ep_utterancesG <- utterances.2sp %>%
  filter(!is_host) %>%
  group_by(episode) %>%
  summarise(full_text = paste(utterance, collapse = " "), .groups = "drop")

# Collapse host names to comma-separated string per episode
host_episode_df <- map_dfr(host.map, ~ tibble(
  episode = .x$episodes,
  host_name = .x$name
)) %>%
  group_by(episode) %>%
  summarise(host_name = paste(unique(host_name), collapse = ", "), .groups = "drop")

# Define safe left_join function to avoid overwriting existing columns
safe_join <- function(df, join_df, by_col, new_col) {
  if (!(new_col %in% colnames(df))) {
    df <- df %>% left_join(join_df, by = by_col)
  } else {
    stop(glue("'{new_col}' already exists"))
  }
  return(df)
}

# Loop through both datasets to attach host metadata and enrich features
for (df_name in c("ep_utterances", "ep_utterancesG")) {
  df <- get(df_name)

  df <- safe_join(df, host_episode_df, "episode", "host_name")
  df <- safe_join(df, episodes, join_by(episode == id), "title")

  # Convert program to factor (if present)
  if ("program" %in% colnames(df)) {
    df$program <- as.factor(df$program)
  }

  # Parse episode_date and create derived date columns (if present)
  if ("episode_date" %in% colnames(df)) {
    df <- df %>%
      mutate(
        episode_date = as.Date(episode_date),
        year = year(episode_date),
        month = month(episode_date, label = TRUE),
        weekday = wday(episode_date, label = TRUE)
      )
  }

  assign(df_name, df, envir = .GlobalEnv)
}


```

# Preprocess Text

```{r}

# create dfm with episode IDs stored as docvars
ep_uttercorp <- corpus(ep_utterances, text_field = "full_text")
docvars(ep_uttercorp, "episode") <- ep_utterances$episode


toks <- tokens(ep_uttercorp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_wordstem() %>%
  tokens_select(pattern = stopwords("en"), selection = "remove")


dfm <- dfm(toks)


```


```{r}

# Define stemmed immigration patterns (or load dynamically)
immigration_stems <- c("immigr", "migrat", "migrant", "refuge", "asylum",
                       "deport", "citizenship", "visa", "undocu")

# Identify matching documents based on keyword hits
doc_index_new <- rowSums(dfm_select(dfm, pattern = immigration_stems)) > 0

# Safely extract the corresponding episode IDs from the DFM
matching_episodes <- docvars(dfm, "episode")[doc_index_new]

# Now subset from the original episode data by ID (not position)
ep_immigration <- ep_utterances %>% filter(episode %in% matching_episodes)


```


```{r}


# Define keyword pattern 
pattern <- paste0(immigration_stems, collapse = "|")

# Function to extract one snippet per episode
get_one_snippet <- function(text, pattern, window = 50) {
  words <- unlist(tokenizers::tokenize_words(text, lowercase = TRUE))
  matches <- which(stringr::str_detect(words, pattern))

  if (length(matches) == 0) return(NA_character_)

  # Build windows around each match
  windows <- lapply(matches, function(i) c(max(1, i - window), min(length(words), i + window)))

  # Merge overlapping windows
  windows <- windows[order(sapply(windows, `[[`, 1))]
  merged <- list()
  current <- windows[[1]]

  for (w in windows[-1]) {
    if (w[1] <= current[2]) {
      current[2] <- max(current[2], w[2])  # extend current window
    } else {
      merged <- append(merged, list(current))
      current <- w
    }
  }
  merged <- append(merged, list(current))

  # Choose one merged snippet at random
  chosen <- merged[[sample(length(merged), 1)]]
  snippet <- paste(words[chosen[1]:chosen[2]], collapse = " ")
  return(snippet)
}

snippets <- sapply(ep_immigration$full_text, get_one_snippet, pattern = pattern)


snippets_df <- data.frame(
  episode = ep_immigration$episode,
  snippet = snippets,
  stringsAsFactors = FALSE
)

# Join snippets back to metadata
immigration <- left_join(snippets_df, ep_immigration, join_by(episode))



```

# Inter-coder Reliability 



```{r}

hc_complete <- read_csv("hc_complete.csv")

all_categories <- c("Security.Threat", "Economic", "Humanitarian.Moral", "Other")


cols_1 <- paste0(all_categories, "1")
cols_2 <- paste0(all_categories, "2")

# ensure one-hot columns are numeric
hc_complete[cols_1] <- lapply(hc_complete[cols_1], as.numeric)
hc_complete[cols_2] <- lapply(hc_complete[cols_2], as.numeric)

#function to extract label from one-hot columns
get_label <- function(row, suffix) {
  selected <- which(row[paste0(all_categories, suffix)] == 1)
  if (length(selected) == 1) {
    return(all_categories[selected])
  } else {
    return(NA)
  }
}

hc_complete$coder1_label <- apply(hc_complete, 1, get_label, suffix = "1")
hc_complete$coder2_label <- apply(hc_complete, 1, get_label, suffix = "2")

hc_complete$coder1_label <- factor(hc_complete$coder1_label, levels = all_categories)
hc_complete$coder2_label <- factor(hc_complete$coder2_label, levels = all_categories)

# confusion matrix
confusion_matrix <- table(hc_complete$coder1_label, hc_complete$coder2_label)
print(confusion_matrix)


#################################################
# making matrix for kripp alpha 

valid_categories <- c("Security.Threat", "Economic", "Humanitarian.Moral", "Other")
category_lookup <- setNames(1:4, valid_categories)

hc_clean <- hc_complete %>%
  filter(coder1_label %in% valid_categories,
         coder2_label %in% valid_categories)

# convert labels to numeric using the lookup
rater_matrix <- data.frame(
  coder1 = category_lookup[hc_clean$coder1_label],
  coder2 = category_lookup[hc_clean$coder2_label]
)

# transpose and run kripp alpha
rater_matrix_t <- t(rater_matrix)
kripp.alpha(rater_matrix_t, method = "nominal")


sum(hc_complete$coder1_label != hc_complete$coder2_label, na.rm = TRUE)

```

#Bayes Classifier



```{r}

if (!"snippet" %in% names(hc_complete)) {
  hc_complete <- hc_complete %>%
    left_join(
      immigration %>% select(episode, snippet),
      by = c("matched_episode" = "episode")
    )
}

# split into train/test sets
set.seed(123)
n <- nrow(hc_complete)
train_index <- sample(seq_len(n), size = floor(0.75 * n))

train_df <- hc_complete[train_index, ]
test_df  <- hc_complete[-train_index, ]


train_corp <- corpus(train_df, text_field = "snippet")
test_corp  <- corpus(test_df,  text_field = "snippet")


train_toks <- tokens(train_corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en"))

test_toks <- tokens(test_corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en"))


train_dfm <- dfm(train_toks)

test_dfm <- dfm(test_toks) %>%
  dfm_match(featnames(train_dfm))


docnames(train_dfm) <- train_df$matched_episode
docnames(test_dfm)  <- test_df$matched_episode

# use coder1_label as factor classification target
train_df <- train_df %>%
  mutate(label = factor(coder1_label))

test_df <- test_df %>%
  mutate(label = factor(coder1_label))

# align DFM rows to metadata
train_dfm <- train_dfm[docnames(train_dfm) %in% train_df$matched_episode, ]
test_dfm  <- test_dfm[docnames(test_dfm) %in% test_df$matched_episode, ]

# Naive Bayes model
nb_model <- textmodel_nb(train_dfm, y = train_df$label)

predicted <- predict(nb_model, newdata = test_dfm)
test_df$predicted <- predicted

# conf matrix
confusion <- table(Predicted = predicted, Actual = test_df$label)
print(confusion)

# Accuracy
accuracy <- sum(diag(confusion)) / sum(confusion)
cat("Accuracy:", round(accuracy, 3), "\n")


```


```{r}

library(ellmer)

chat <- chat_google_gemini()

classify_immigration_frame <- function(text) {
  # Pause between calls to avoid rate-limiting
  Sys.sleep(1.5)

  prompt <- paste(
    "Classify the following immigration-related text into one of four frames:\n\n",

    "1. Security/Threat: Immigration as a threat to safety, legality, or national security.\n",
    "   Keywords: illegal, deportation, border wall, crime, ICE, terrorism, smuggling\n",
    
    "2. Economic: Immigration framed through jobs, wages, taxes, or labor market impact.\n",
    "   Keywords: jobs, labor, taxes, economy, employment, cost, contribution\n",
    
    "3. Humanitarian/Moral: Emphasizes suffering, dignity, or moral obligation.\n",
    "   Keywords: asylum, refugee, family, trauma, discrimination, DACA, compassion\n",
    
    
    "4. Other: Neutral, off-topic, or doesn't fit the above categories.\n\n",
    
    "Sometimes, a text may appear sympathetic but still evoke a Security/Threat frame. For example, a story that shows immigrants fleeing violence may still associate them with terrorism or crime in the reader’s mind. Classify such cases as Security/Threat, not Humanitarian."

 "Examples:\n",
"- 'ICE detained 12 undocumented migrants at the border.' → Security/Threat\n",
"- 'A migrant fled gang violence and drug cartels.' → Security/Threat\n",   # <= key!
"- 'Immigrants contribute billions in taxes.' → Economic\n",
"- 'A child was separated from her parents at the border.' → Humanitarian/Moral\n",
"- 'The episode focused on campaign strategy.' → Other\n\n",


    "Now classify this:\n",
    "Text: ", text, "\n",
    "Respond with ONLY ONE of: Security/Threat, Economic, Humanitarian/Moral, Other"
  )

  result <- tryCatch({
    response <- chat$chat(prompt)
    response_clean <- tolower(response)

    if (grepl("security", response_clean)) {
      "Security/Threat"
    } else if (grepl("economic", response_clean)) {
      "Economic"
    } else if (grepl("humanitarian|moral", response_clean)) {
      "Humanitarian/Moral"
    } else if (grepl("other", response_clean)) {
      "Other"
    } else {
      "Unclear"
    }
  }, error = function(e) {
    message("API error: ", e$message)
    return(NA)
  })

  return(result)
}



hc_complete$gemini_label <- sapply(hc_complete$snippet, classify_immigration_frame)



```




