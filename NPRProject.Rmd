---
title: "NPR Project"
output: html_document
date: "2025-05-01"
---

```{r, results = 'hide', message = FALSE, warning = FALSE}



library(tidyverse)
library(readr)
library(jsonlite)
library(purrr)
library(lubridate)
library(quanteda)  
library(quanteda.textmodels)
library(here)
library(tokenizers)
library(topicmodels)
library(syuzhet)
library(caret)
library(irr)
library(SnowballC)
library(ellmer)
library(data.table)
library(e1071)
library(randomForest)
library(glmnet)


# pull files 
csvfiles <- list.files(path = here("archive"), pattern = "\\.csv$", full.names = TRUE)
jsonfiles <- list.files(path = here("archive"), pattern = "\\.json$", full.names = TRUE)

names(csvfiles) <- make.names(tools::file_path_sans_ext(basename(csvfiles)))
names(jsonfiles) <- make.names(tools::file_path_sans_ext(basename(jsonfiles)))

# load files w anon fuction 
walk(names(csvfiles), ~ assign(.x, read_csv(csvfiles[[.x]]), envir = .GlobalEnv))
walk(names(jsonfiles), ~ assign(.x, fromJSON(jsonfiles[[.x]], flatten = TRUE), envir = .GlobalEnv))


```

# Cleaning

```{r clean, results = "hide"}


# Collapse to 1 observation per episode (full transcript and guest-only)
ep_utterances <- utterances.2sp %>%
  group_by(episode) %>%
  summarise(full_text = paste(utterance, collapse = " "), .groups = "drop")

ep_utterancesG <- utterances.2sp %>%
  filter(!is_host) %>%
  group_by(episode) %>%
  summarise(full_text = paste(utterance, collapse = " "), .groups = "drop")

# Collapse host names to comma-separated string per episode
host_episode_df <- map_dfr(host.map, ~ tibble(
  episode = .x$episodes,
  host_name = .x$name
)) %>%
  group_by(episode) %>%
  summarise(host_name = paste(unique(host_name), collapse = ", "), .groups = "drop")

# Define safe left_join function to avoid overwriting existing columns
safe_join <- function(df, join_df, by_col, new_col) {
  if (!(new_col %in% colnames(df))) {
    df <- df %>% left_join(join_df, by = by_col)
  } else {
    stop(glue("'{new_col}' already exists"))
  }
  return(df)
}

# Loop through both datasets to attach host metadata and enrich features
for (df_name in c("ep_utterances", "ep_utterancesG")) {
  df <- get(df_name)

  df <- safe_join(df, host_episode_df, "episode", "host_name")
  df <- safe_join(df, episodes, join_by(episode == id), "title")

  # Convert program to factor (if present)
  if ("program" %in% colnames(df)) {
    df$program <- as.factor(df$program)
  }

  # Parse episode_date and create derived date columns (if present)
  if ("episode_date" %in% colnames(df)) {
    df <- df %>%
      mutate(
        episode_date = as.Date(episode_date),
        year = year(episode_date),
        month = lubridate::month(episode_date, label = TRUE),
        weekday = lubridate::wday(episode_date, label = TRUE)
      )
  }

  assign(df_name, df, envir = .GlobalEnv)
}


```

# Preprocess Text

```{r create dfm,  results = 'hide'}

# create dfm with episode IDs stored as docvars
ep_uttercorp <- corpus(ep_utterances, text_field = "full_text")
docvars(ep_uttercorp, "episode") <- ep_utterances$episode


toks <- tokens(ep_uttercorp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_wordstem() %>%
  tokens_select(pattern = stopwords("en"), selection = "remove")


dfm <- dfm(toks)


```

```{r immigration subset,  results = 'hide'}

# Define stemmed immigration patterns (or load dynamically)
immigration_stems <- c("immigr", "migrat", "migrant", "refuge", "asylum",
                       "deport", "citizenship", "visa", "undocu")

# Identify matching documents based on keyword hits
doc_index_new <- rowSums(dfm_select(dfm, pattern = immigration_stems)) > 0

# Safely extract the corresponding episode IDs from the DFM
matching_episodes <- docvars(dfm, "episode")[doc_index_new]

# Now subset from the original episode data by ID (not position)
ep_immigration <- ep_utterances %>% filter(episode %in% matching_episodes)


```

```{r snippet create,  results = 'hide'}


# Define keyword pattern 
pattern <- paste0(immigration_stems, collapse = "|")

# Function to extract one snippet per episode
get_one_snippet <- function(text, pattern, window = 50) {
  words <- unlist(tokenizers::tokenize_words(text, lowercase = TRUE))
  matches <- which(stringr::str_detect(words, pattern))

  if (length(matches) == 0) return(NA_character_)

  # Build windows around each match
  windows <- lapply(matches, function(i) c(max(1, i - window), min(length(words), i + window)))

  # Merge overlapping windows
  windows <- windows[order(sapply(windows, `[[`, 1))]
  merged <- list()
  current <- windows[[1]]

  for (w in windows[-1]) {
    if (w[1] <= current[2]) {
      current[2] <- max(current[2], w[2])  # extend current window
    } else {
      merged <- append(merged, list(current))
      current <- w
    }
  }
  merged <- append(merged, list(current))

  # Choose one merged snippet at random
  chosen <- merged[[sample(length(merged), 1)]]
  snippet <- paste(words[chosen[1]:chosen[2]], collapse = " ")
  return(snippet)
}

snippets <- sapply(ep_immigration$full_text, get_one_snippet, pattern = pattern)


snippets_df <- data.frame(
  episode = ep_immigration$episode,
  snippet = snippets,
  stringsAsFactors = FALSE
)

# Join snippets back to metadata
immigration <- left_join(snippets_df, ep_immigration, join_by(episode))


```

# Inter-coder Reliability

```{r intercoder reliability }

invisible(hc_complete <- read_csv("hc_complete.csv", show_col_types = FALSE))


all_categories <- c("Security.Threat", "Economic", "Humanitarian.Moral", "Other")


cols_1 <- paste0(all_categories, "1")
cols_2 <- paste0(all_categories, "2")

# ensure one-hot columns are numeric
hc_complete[cols_1] <- lapply(hc_complete[cols_1], as.numeric)
hc_complete[cols_2] <- lapply(hc_complete[cols_2], as.numeric)

#function to extract label from one-hot columns
get_label <- function(row, suffix) {
  selected <- which(row[paste0(all_categories, suffix)] == 1)
  if (length(selected) == 1) {
    return(all_categories[selected])
  } else {
    return(NA)
  }
}

hc_complete$coder1_label <- apply(hc_complete, 1, get_label, suffix = "1")
hc_complete$coder2_label <- apply(hc_complete, 1, get_label, suffix = "2")

hc_complete$coder1_label <- factor(hc_complete$coder1_label, levels = all_categories)
hc_complete$coder2_label <- factor(hc_complete$coder2_label, levels = all_categories)

# confusion matrix
confusion_matrix <- table(hc_complete$coder1_label, hc_complete$coder2_label)
print(confusion_matrix)


#################################################
# making matrix for kripp alpha 

valid_categories <- c("Security.Threat", "Economic", "Humanitarian.Moral", "Other")
category_lookup <- setNames(1:4, valid_categories)

hc_clean <- hc_complete %>%
  filter(coder1_label %in% valid_categories,
         coder2_label %in% valid_categories)

# convert labels to numeric using the lookup
rater_matrix <- data.frame(
  coder1 = category_lookup[hc_clean$coder1_label],
  coder2 = category_lookup[hc_clean$coder2_label]
)

# transpose and run kripp alpha
rater_matrix_t <- t(rater_matrix)
kripp.alpha(rater_matrix_t, method = "nominal")


```

# Bayes Classifier

```{r bayes}

if (!"snippet" %in% names(hc_complete)) {
  hc_complete <- hc_complete %>%
    left_join(
      immigration %>% select(episode, snippet),
      by = c("matched_episode" = "episode")
    )
}


# LOOCV

set.seed(123)
n <- nrow(hc_complete)

# Prepare full corpus and tokens once
full_corp <- corpus(hc_complete, text_field = "snippet")
full_toks <- tokens(full_corp, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en")) %>%
  tokens_ngrams(n = 1:2)

full_dfm <- dfm(full_toks) %>%
  dfm_trim(min_termfreq = 5) %>%
  dfm_weight(scheme = "prop")


# True labels and matched docnames
true_labels <- factor(hc_complete$coder1_label)
doc_ids <- hc_complete$matched_episode

# Store predictions
all_predictions <- character(n)

for (i in 1:n) {
  # Split LOOCV train/test
  train_dfm <- full_dfm[-i, ]
  test_dfm <- full_dfm[i, , drop = FALSE]
  
  train_labels <- true_labels[-i]
  
  # Match features
  test_dfm <- dfm_match(test_dfm, featnames(train_dfm))
  
  # Train Naive Bayes
  nb_model <- textmodel_nb(train_dfm, y = train_labels)

  # Predict
  pred <- predict(nb_model, newdata = test_dfm)
  all_predictions[i] <- as.character(unname(pred))
}

# Add predictions back to data
hc_complete$predicted <- factor(all_predictions, levels = levels(true_labels))

# Confusion matrix
conf_matrix <- confusionMatrix(hc_complete$predicted, true_labels)

conf_matrix

# Output precision and recall per class
print(conf_matrix$byClass[, c("Precision", "Recall")])


table(true_labels)
table(hc_complete$predicted)


```

```{r}


# === Load GloVe embeddings ===
glove_file <- "glove.6B.100d.txt"
glove_raw <- fread(
  glove_file,
  header = FALSE,
  quote = "",
  colClasses = c("character", rep("numeric", 100))
)
setnames(glove_raw, c("word", paste0("V", 1:100)))

# === Prepare document embeddings ===
tokenized <- strsplit(tolower(hc_complete$snippet), "\\s+")

doc_embeddings <- t(sapply(tokenized, function(words) {
  matched <- glove_raw[glove_raw$word %in% words]
  if (nrow(matched) > 0) {
    colMeans(as.matrix(matched[, -1, with = FALSE]))
  } else {
    rep(0, 100)
  }
}))

# === Reduce dimensionality ===
pca <- prcomp(doc_embeddings, center = TRUE, scale. = TRUE)
reduced_embeddings <- pca$x[, 1:30]  # Using top 30 PCs

# === Set up labels ===
true_labels <- factor(hc_complete$coder1_label)
n_total <- nrow(hc_complete)

# === Train/Test Split (80 docs) ===
set.seed(123)
if (n_total <= 80) {
  train_idx <- sample(seq_len(n_total), floor(0.8 * n_total))
} else {
  train_idx <- sample(seq_len(n_total), 80)
}
test_idx <- setdiff(seq_len(n_total), train_idx)

train_x <- reduced_embeddings[train_idx, ]
train_y <- true_labels[train_idx]
test_x <- reduced_embeddings[test_idx, ]
test_y <- true_labels[test_idx]

# === Calculate class weights ===
class_counts <- table(train_y)
inv_weights <- sum(class_counts) / (length(class_counts) * class_counts)
inv_weights <- as.list(inv_weights)
names(inv_weights) <- names(class_counts)

cat("Class weights:\n")
print(inv_weights)

# === Train weighted SVM ===
weighted_svm <- svm(
  x = train_x,
  y = train_y,
  kernel = "linear",
  class.weights = inv_weights
)

# === Predict and evaluate ===
weighted_pred <- predict(weighted_svm, test_x)
weighted_conf <- confusionMatrix(weighted_pred, test_y)

cat("\n=== Weighted SVM Performance ===\n")
print(weighted_conf)
print(weighted_conf$byClass[, c("Precision", "Recall")])

# === Train unweighted SVM ===
unweighted_svm <- svm(
  x = train_x,
  y = train_y,
  kernel = "linear"
)

# === Predict and evaluate ===
unweighted_pred <- predict(unweighted_svm, test_x)
unweighted_conf <- confusionMatrix(unweighted_pred, test_y)

cat("\n=== Unweighted SVM Performance ===\n")
print(unweighted_conf)
print(unweighted_conf$byClass[, c("Precision", "Recall")])


```

# LASSO

```{r}


# === Set up input ===
# (Already reduced embeddings from PCA)
X <- reduced_embeddings
Y <- true_labels

# Encode as numeric matrix
Y_numeric <- as.numeric(Y)  # glmnet needs numeric class labels

# === Train/Test Split (same logic) ===
set.seed(123)
if (nrow(X) <= 80) {
  train_idx <- sample(seq_len(nrow(X)), floor(0.8 * nrow(X)))
} else {
  train_idx <- sample(seq_len(nrow(X)), 80)
}
test_idx <- setdiff(seq_len(nrow(X)), train_idx)

train_x <- X[train_idx, ]
train_y <- Y[train_idx]
test_x <- X[test_idx, ]
test_y <- Y[test_idx]

# === Fit LASSO Multinomial Logistic Regression ===
lasso_model <- cv.glmnet(
  x = train_x,
  y = train_y,
  family = "multinomial",
  alpha = 1  # LASSO penalty
)

# === Predict ===
predictions <- predict(lasso_model, newx = test_x, s = "lambda.min", type = "class")
predictions <- factor(predictions, levels = levels(Y))

# === Evaluate ===
conf_matrix <- confusionMatrix(predictions, test_y)
print(conf_matrix)
print(conf_matrix$byClass[, c("Precision", "Recall")])




```

# Random Forest

```{r}


hc_complete <- hc_complete %>% 
  left_join(immigration %>%  
              select(episode, lda_based_frame), 
            join_by(matched_episode== episode))



# Combine data
train_data <- data.frame(train_x)
train_data$label <- train_y

# Upsample minority classes
set.seed(123)
balanced_data <- upSample(x = train_x, y = train_y)

# Train Random Forest
rf_model <- randomForest(x = balanced_data[, -ncol(balanced_data)], y = balanced_data$Class, ntree = 500)

# Predict on test set
predictions <- predict(rf_model, test_x)

# Evaluate
conf_matrix <- confusionMatrix(predictions, test_y)
print(conf_matrix)
print(conf_matrix$byClass[, c("Precision", "Recall")])


```

# LLM

```{r gemini, eval = F }
library(ellmer)


chat <- ellmer::chat_google_gemini()

classify_immigration_frame <- function(text) {
  Sys.sleep(5)  # avoid rate limits
  
  prompt <- paste(
    "Classify the following immigration-related text into one of four frames:\n\n",
    
    "Security/Threat Frame\n",
    "Articles that primarily frame immigration as a threat to national security, public safety, or the legal order. Even if a story seems sympathetic, if it evokes associations with security or crime (e.g., fleeing violence from cartels), classify as Security/Threat.\n",
    "Focus on illegal entry, smuggling, or border crossings\n",
    "Coverage of immigration enforcement (e.g., ICE raids, deportations)\n",
    "Crime or terrorism tied to immigrants or asylum seekers\n",
    "National security concerns or protective measures\n",
    "Political rhetoric focused on law and order\n",
    "Keywords: illegal, deportation, border wall, crime, ICE, security, terrorism, smuggling, cartels, public safety\n\n",
    
    "Economic Frame\n",
    "Articles that focus on the financial or labor market implications of immigration.\n",
    "Immigrants as workers in specific industries\n",
    "Central focus on appropriations/funding of immigration issues, even if the funding debate leads speakers to make arguments that fall into one of the other categories.\n",
    "Impact on wages, employment, housing, or taxes\n",
    "Economic contributions (e.g., entrepreneurship, GDP, remittances)\n",
    "Business demand for migrant labor or work visas\n",
    "Fiscal burden or benefit arguments\n",
    "Keywords: jobs, labor, taxes, contribution, costs, economic growth, shortages, employment, undocumented workers\n\n",
    
    "Humanitarian/Moral Frame\n",
    "Articles that highlight the lived experiences, rights, or moral standing of immigrants and refugees.\n",
    "Stories of asylum seekers, refugees, or displaced families\n",
    "Discussion of living conditions is the strongest predictor of this frame. Next;\n",
    "Focus on human suffering, trauma, or discrimination\n",
    "Coverage of family separation, detainment, or access to services\n",
    "Discussions of inclusion, dignity, or moral responsibility\n",
    "Civil society efforts to aid or defend migrant\n",
    "Keywords: asylum, refugee, family, rights, trauma, detention, inclusion, discrimination, DACA, compassion, migrant children\n\n",
    
    "Other\n",
    "Mention of immigration is offhand, not the major focus of the piece—for example, coverage of a session of congress where immigration is a topic of focus, but immigration is merely tangential. Or, discussion of presidential approval rates, and immigration is mentioned as a topic influencing the results, but is not the focus of the piece. Finally, snippets generally not relevant to other categories should fall here.\n",
    "Broad overview of political position/environment that only tangentially mentions immigration.\n\n",
    
    "Now classify this:\n",
    "Text: ", text, "\n",
    "Respond with ONLY ONE of: Security/Threat, Economic, Humanitarian/Moral, Other"
  )
  
  result <- tryCatch({
    response <- chat$chat(prompt)
    response_clean <- tolower(response)
    
    if (grepl("security", response_clean)) {
      "Security/Threat"
    } else if (grepl("economic", response_clean)) {
      "Economic"
    } else if (grepl("humanitarian|moral", response_clean)) {
      "Humanitarian/Moral"
    } else if (grepl("other", response_clean)) {
      "Other"
    } else {
      "Unclear"
    }
  }, error = function(e) {
    message("API error: ", e$message)
    return(NA)
  })
  
  return(result)
}


hc_complete$gemini_label <- sapply(immigration$snippet, classify_immigration_frame)


```

```{r gemini results, eval = F}

# Calculate precision & recall
hc_complete$gemini_label <- gsub("/", ".", hc_complete$gemini_label)


names(hc_complete)
true_labels <- factor(hc_complete$coder1_label)
predicted_labels <- factor(hc_complete$gemini_label, levels = levels(true_labels))

conf_matrix <- confusionMatrix(predicted_labels, true_labels)

conf_matrix
# Print precision and recall table
print(conf_matrix$byClass[, c("Precision", "Recall")])



```


```{r}

#Batched prompt to classify entire immigration df 

chat <- ellmer::chat_google_gemini()

classify_immigration_batch <- function(text_batch) {
  Sys.sleep(7) 

  prompt <- paste(
    "Classify the following 3 immigration-related texts separately into one of four frames:\n\n",
    
    "Security/Threat Frame\n",
    "Articles that primarily frame immigration as a threat to national security, public safety, or the legal order. Even if a story seems sympathetic, if it evokes associations with security or crime (e.g., fleeing violence from cartels), classify as Security/Threat.\n",
    "Focus on illegal entry, smuggling, or border crossings\n",
    "Coverage of immigration enforcement (e.g., ICE raids, deportations)\n",
    "Crime or terrorism tied to immigrants or asylum seekers\n",
    "National security concerns or protective measures\n",
    "Political rhetoric focused on law and order\n",
    "Keywords: illegal, deportation, border wall, crime, ICE, security, terrorism, smuggling, cartels, public safety\n\n",
    
    "Economic Frame\n",
    "Articles that focus on the financial or labor market implications of immigration.\n",
    "Immigrants as workers in specific industries\n",
    "Central focus on appropriations/funding of immigration issues, even if the funding debate leads speakers to make arguments that fall into one of the other categories.\n",
    "Impact on wages, employment, housing, or taxes\n",
    "Economic contributions (e.g., entrepreneurship, GDP, remittances)\n",
    "Business demand for migrant labor or work visas\n",
    "Fiscal burden or benefit arguments\n",
    "Keywords: jobs, labor, taxes, contribution, costs, economic growth, shortages, employment, undocumented workers\n\n",
    
    "Humanitarian/Moral Frame\n",
    "Articles that highlight the lived experiences, rights, or moral standing of immigrants and refugees.\n",
    "Stories of asylum seekers, refugees, or displaced families\n",
    "Discussion of living conditions is the strongest predictor of this frame. Next;\n",
    "Focus on human suffering, trauma, or discrimination\n",
    "Coverage of family separation, detainment, or access to services\n",
    "Discussions of inclusion, dignity, or moral responsibility\n",
    "Civil society efforts to aid or defend migrant\n",
    "Keywords: asylum, refugee, family, rights, trauma, detention, inclusion, discrimination, DACA, compassion, migrant children\n\n",
    
    "Other\n",
    "Mention of immigration is offhand, not the major focus of the piece—for example, coverage of a session of congress where immigration is a topic of focus, but immigration is merely tangential. Or, discussion of presidential approval rates, and immigration is mentioned as a topic influencing the results, but is not the focus of the piece. Finally, snippets generally not relevant to other categories should fall here.\n",
    "Broad overview of political position/environment that only tangentially mentions immigration.\n\n",
    
    "Now classify these:\n",
    paste0("Text 1: ", text_batch[1], "\n"),
    paste0("Text 2: ", text_batch[2], "\n"),
    paste0("Text 3: ", text_batch[3], "\n"),
    "Respond ONLY with three lines:\n1) [frame]\n2) [frame]\n3) [frame]"
  )

  result <- tryCatch({
    response <- chat$chat(prompt)
    response_lines <- strsplit(response, "\n")[[1]]
    
    frames <- sapply(response_lines, function(line) {
      line_clean <- tolower(gsub("^\\d+\\)\\s*", "", line))
      if (grepl("security", line_clean)) {
        "Security/Threat"
      } else if (grepl("economic", line_clean)) {
        "Economic"
      } else if (grepl("humanitarian|moral", line_clean)) {
        "Humanitarian/Moral"
      } else if (grepl("other", line_clean)) {
        "Other"
      } else {
        "Unclear"
      }
    })

    return(frames)
    
  }, error = function(e) {
    message("API error on batch: ", e$message)
    return(rep(NA, 3))
  })
  
  return(result)
}

texts <- immigration$snippet

# Split into batches of 3
batches <- split(texts, ceiling(seq_along(texts) / 3))

# Load existing results if they exist
if (exists("final_results")) {
  processed_indices <- final_results$index
  start_batch <- ceiling(max(processed_indices) / 3) + 1
  cat("Resuming from batch", start_batch, "of", length(batches), "\n")
  
  all_results <- split(final_results, ceiling(final_results$index / 3))
} else {
  start_batch <- 1
  all_results <- list()
}


for (i in start_batch:length(batches)) {
  batch <- batches[[i]]
  
  # Ensure the batch has exactly 3 items 
  while (length(batch) < 3) {
    batch <- c(batch, "")
  }
  
  cat("Processing batch", i, "of", length(batches), "\n")
  
  frames <- classify_immigration_batch(batch)
  
  indices <- ((i - 1) * 3 + 1):min(i * 3, length(texts))
  
  all_results[[i]] <- data.frame(
    index = indices,
    frame = frames[1:length(indices)]  # trim extra if padded
  )

  
  if (i %% 10 == 0) {
    saveRDS(do.call(rbind, all_results), file = "checkpoint_results.rds")
    cat("Checkpoint saved at batch", i, "\n")
  }
}

final_results <- do.call(rbind, all_results)
immigration$gemini_label <- final_results$frame





# NA handling
na_indices <- final_results$index[is.na(final_results$frame)]

cat("Number of NA entries:", length(na_indices), "\n")


na_batches <- unique(ceiling(na_indices / 3))

cat("Number of batches to re-run:", length(na_batches), "\n")


for (i in na_batches) {
  batch <- batches[[i]]
  
  while (length(batch) < 3) {
    batch <- c(batch, "")
  }
  
  cat("Re-running batch", i, "of", length(batches), "\n")
  
  frames <- classify_immigration_batch(batch)
  
  indices <- ((i - 1) * 3 + 1):min(i * 3, length(texts))
  
  # Update final_results in place
  final_results$frame[final_results$index %in% indices] <- frames[1:length(indices)]
  
  # Optional: save checkpoint after each retry
  saveRDS(final_results, "checkpoint_results_retry.rds")
}



```
